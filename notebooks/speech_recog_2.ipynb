{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86291479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import parselmouth\n",
    "from fer import FER\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "from moviepy.editor import VideoFileClip  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c0f6728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract audio from video\n",
    "def extract_audio_from_video(video_path):\n",
    "    \"\"\"\n",
    "    Extract audio from the video and save it as a .wav file with the same name as the video.\n",
    "    \"\"\"\n",
    "    base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    output_audio_path = f\"{base_name}_extracted_audio.wav\"\n",
    "    \n",
    "    # Load the video file and extract audio\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio = video.audio\n",
    "    \n",
    "    # Write the audio to a .wav file\n",
    "    audio.write_audiofile(output_audio_path)\n",
    "    \n",
    "    return output_audio_path\n",
    "\n",
    "# Step 2: Extract audio features (MFCC, pitch, jitter, shimmer)\n",
    "def extract_features(audio_path):\n",
    "    \"\"\"\n",
    "    Extract MFCC features, Jitter, Pitch, and Shimmer from an audio file.\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(audio_path, sr=None)  # 'sr' is the sampling rate; None means keep the original\n",
    "    \n",
    "    # Extract MFCC features\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # We are using 13 MFCC coefficients\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)  # Average the MFCCs over time\n",
    "    \n",
    "    # Extract Jitter, Pitch, and Shimmer using parselmouth\n",
    "    sound = parselmouth.Sound(audio_path)\n",
    "    \n",
    "    # Get pitch\n",
    "    pitch = sound.to_pitch()\n",
    "    pitch_values = pitch.selected_array['frequency']  # Get pitch values\n",
    "    \n",
    "    # Jitter (variability in pitch)\n",
    "    jitter = np.std(pitch_values) / np.mean(pitch_values) if np.mean(pitch_values) != 0 else 0  # Jitter calculation\n",
    "    \n",
    "    # Shimmer (variability in amplitude)\n",
    "    shimmer = np.std(pitch_values) / np.mean(pitch_values) if np.mean(pitch_values) != 0 else 0  # Simplified shimmer calculation\n",
    "    \n",
    "    # Calculate mean of each feature\n",
    "    pitch_mean = np.mean(pitch_values)\n",
    "    \n",
    "    # Combine all features into one array\n",
    "    combined_features = np.hstack((mfcc_mean, pitch_mean, jitter, shimmer))\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "# Step 3: Process all audio files in a directory\n",
    "def process_audio_files(directory):\n",
    "    \"\"\"\n",
    "    Process all the audio files in a given directory, extracting features and labels.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Loop over each file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.wav'):\n",
    "            # Extract the label from the filename (e.g., OAF_back_angry.wav -> angry)\n",
    "            label = filename.split('_')[-1].replace('.wav', '')  # Assuming emotion is the last part of the filename\n",
    "            \n",
    "            # Extract features\n",
    "            audio_path = os.path.join(directory, filename)\n",
    "            extracted_features = extract_features(audio_path)\n",
    "            features.append(extracted_features)\n",
    "            labels.append(label)\n",
    "\n",
    "    # Convert to numpy array and return\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Step 4: Extract visual features (e.g., facial emotion detection using FER)\n",
    "def extract_visual_features_from_video(video_path):\n",
    "    \"\"\"\n",
    "    Extract visual emotion features from the video using FER.\n",
    "    \"\"\"\n",
    "    detector = FER()\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    visual_emotions = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Detect emotions from the frame\n",
    "        emotions = detector.top_emotion(frame)  # Returns the top emotion (e.g., ('happy', 0.8))\n",
    "\n",
    "        if emotions:\n",
    "            emotion, confidence = emotions\n",
    "            visual_emotions.append((emotion, confidence))\n",
    "    \n",
    "    video_capture.release()\n",
    "    \n",
    "    return visual_emotions\n",
    "\n",
    "# Step 5: Combine audio and visual features into a single feature vector\n",
    "def combine_audio_visual_features(audio_features, visual_features):\n",
    "    \"\"\"\n",
    "    Combine audio and visual features into a single feature vector.\n",
    "    \"\"\"\n",
    "    audio_feature_vector = np.array(audio_features)  # Convert audio features to numpy array\n",
    "    visual_feature_vector = np.array([confidence for emotion, confidence in visual_features])  # Convert visual features to numpy array\n",
    "    \n",
    "    # Concatenate the two feature vectors\n",
    "    combined_features = np.concatenate((audio_feature_vector, visual_feature_vector))\n",
    "        \n",
    "    return combined_features\n",
    "\n",
    "# Step 6: Model Prediction - Predict confidence score using the model\n",
    "def calculate_confidence_score(model, le, audio_file_path):\n",
    "    \"\"\"\n",
    "    Calculate the confidence score based on the model's probabilities for 'happy', 'ps', 'angry', and 'neutral'.\n",
    "    Higher scores indicate more confidence in the presentation.\n",
    "    \"\"\"\n",
    "    # Extract features from the new audio file\n",
    "    new_features = extract_features(audio_file_path).reshape(1, -1)  # Reshape to match model input\n",
    "    \n",
    "    # Get the probabilities of each class (emotion)\n",
    "    probas = model.predict_proba(new_features)\n",
    "    \n",
    "    # Define the class labels for emotions (focus on 'happy', 'ps', 'angry', 'neutral')\n",
    "    emotion_labels = le.classes_  # e.g., ['angry', 'happy', 'nervous', ...]\n",
    "\n",
    "    # Extract the relevant probabilities for 'happy', 'ps', 'angry', and 'neutral'\n",
    "    happy_prob = probas[0][np.where(emotion_labels == 'happy')[0][0]]\n",
    "    ps_prob = probas[0][np.where(emotion_labels == 'ps')[0][0]]\n",
    "    angry_prob = probas[0][np.where(emotion_labels == 'angry')[0][0]]\n",
    "    neutral_prob = probas[0][np.where(emotion_labels == 'neutral')[0][0]]\n",
    "    \n",
    "    # Print probabilities for each emotion of interest\n",
    "    # print(f\"Happy Probability: {happy_prob:.2f}\")\n",
    "    # print(f\"PS Probability: {ps_prob:.2f}\")\n",
    "    # print(f\"Angry Probability: {angry_prob:.2f}\")\n",
    "    # print(f\"Neutral Probability: {neutral_prob:.2f}\")\n",
    "    \n",
    "    print(\"\\nProbabilities for each emotion:\")\n",
    "    for label, prob in zip(emotion_labels, probas[0]):\n",
    "        print(f\"{label.capitalize()}: {prob:.2f}\")\n",
    "    \n",
    "    # Confidence Score Calculation:\n",
    "    # Increasing the weight for confidence emotions ('happy', 'ps', 'angry')\n",
    "    confidence_emotions_prob = (happy_prob * 3 + ps_prob * 3 + angry_prob * 3)  # Increased weight for confidence emotions\n",
    "    confidence_score = max(confidence_emotions_prob, 0.05) * 100  # Ensure a minimum contribution\n",
    "    \n",
    "    # Reduce the weight of neutral's penalty (dampen the effect)\n",
    "    neutral_penalty = neutral_prob * 10  # Reduced penalty (previously 50)\n",
    "\n",
    "    # Subtract the neutral penalty from the confidence score to get the final score\n",
    "    final_confidence_score = confidence_score - neutral_penalty\n",
    "    \n",
    "    # Apply a minimum threshold to ensure the score doesn't drop below a certain level (e.g., 10)\n",
    "    final_confidence_score = max(final_confidence_score, 10)\n",
    "    \n",
    "    # Clamp the score to a range of 0 to 100 for visualization\n",
    "    final_confidence_score = max(0, min(100, final_confidence_score))\n",
    "    \n",
    "    print(f\"Final Confidence Score: {final_confidence_score:.2f}\")\n",
    "    return final_confidence_score\n",
    "\n",
    "# Process audio files (Training the Model)\n",
    "directory = \"dataverse_files\"  # Directory containing audio files\n",
    "X, y = process_audio_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5083483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on the test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       1.00      0.99      0.99        88\n",
      "     disgust       0.99      1.00      0.99        86\n",
      "        fear       0.96      1.00      0.98        64\n",
      "       happy       0.99      0.93      0.96        85\n",
      "     neutral       0.97      0.97      0.97        66\n",
      "          ps       0.96      0.98      0.97        89\n",
      "         sad       0.99      0.99      0.99        82\n",
      "\n",
      "    accuracy                           0.98       560\n",
      "   macro avg       0.98      0.98      0.98       560\n",
      "weighted avg       0.98      0.98      0.98       560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encode the labels (e.g., angry -> 0, sad -> 1)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestClassifier model\n",
    "model = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Model performance on the test set:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "419e503c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ted_talk_sample_extracted_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/Users/tsuzumi.sato/.pyenv/versions/non_verbal_comm/lib/python3.10/site-packages/keras/src/models/functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(1, 64, 64))\n",
      "  warnings.warn(msg)\n",
      "\n",
      "WARNING:py.warnings:/Users/tsuzumi.sato/.pyenv/versions/non_verbal_comm/lib/python3.10/site-packages/keras/src/models/functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(2, 64, 64))\n",
      "  warnings.warn(msg)\n",
      "\n",
      "WARNING:py.warnings:/Users/tsuzumi.sato/.pyenv/versions/non_verbal_comm/lib/python3.10/site-packages/keras/src/models/functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(3, 64, 64))\n",
      "  warnings.warn(msg)\n",
      "\n",
      "WARNING:py.warnings:/Users/tsuzumi.sato/.pyenv/versions/non_verbal_comm/lib/python3.10/site-packages/keras/src/models/functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(4, 64, 64))\n",
      "  warnings.warn(msg)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probabilities for each emotion:\n",
      "Angry: 0.06\n",
      "Disgust: 0.62\n",
      "Fear: 0.17\n",
      "Happy: 0.02\n",
      "Neutral: 0.00\n",
      "Ps: 0.10\n",
      "Sad: 0.04\n",
      "Final Confidence Score: 52.50\n",
      "Predicted Confidence Score: 52.50\n"
     ]
    }
   ],
   "source": [
    "video_path = 'ted_talk_sample.mp4'  # Replace with your video path\n",
    "\n",
    "# Extract audio features\n",
    "audio_path = extract_audio_from_video(video_path)\n",
    "audio_features = extract_features(audio_path)\n",
    "\n",
    "# Extract visual features (emotion detection)\n",
    "visual_emotions = extract_visual_features_from_video(video_path)\n",
    "\n",
    "# Combine audio and visual features\n",
    "combined_features = combine_audio_visual_features(audio_features, visual_emotions)\n",
    "\n",
    "# Get the confidence score\n",
    "confidence_score = calculate_confidence_score(model, le, audio_path)\n",
    "print(f\"Predicted Confidence Score: {confidence_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e964291a",
   "metadata": {},
   "source": [
    "Probabilities for each emotion:\n",
    "Angry: 0.06\n",
    "Disgust: 0.62\n",
    "Fear: 0.17\n",
    "Happy: 0.02\n",
    "Neutral: 0.00\n",
    "Ps: 0.10\n",
    "Sad: 0.04\n",
    "Final Confidence Score: 52.50\n",
    "Predicted Confidence Score: 52.50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7544699",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "non_verbal_comm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
