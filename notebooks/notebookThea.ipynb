{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666d4e44",
   "metadata": {},
   "source": [
    "## Model Body Language "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e50f884",
   "metadata": {},
   "source": [
    "Data Exploration - Thea "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "693ff663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(69878) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: absl-py in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from mediapipe) (2.2.2)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from mediapipe) (25.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from mediapipe) (0.6.1)\n",
      "Requirement already satisfied: jaxlib in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from mediapipe) (0.6.1)\n",
      "Requirement already satisfied: matplotlib in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from mediapipe) (3.10.3)\n",
      "Requirement already satisfied: numpy<2 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from mediapipe) (4.25.7)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: sentencepiece in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from jax->mediapipe) (1.15.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from matplotlib->mediapipe) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from matplotlib->mediapipe) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from matplotlib->mediapipe) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from matplotlib->mediapipe) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/theaalfon/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128df680",
   "metadata": {},
   "source": [
    "## Initialize MediaPipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f087ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9e634",
   "metadata": {},
   "source": [
    "Initialize BlazePose MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e25bc514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748420889.623221 36351713 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1748420889.713520 36351886 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748420889.725757 36351886 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose_model = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618cace",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fd9c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Normalize utility ---\n",
    "def normalize(value, min_val, max_val):\n",
    "    return round(10 * np.clip((value - min_val) / (max_val - min_val), 0, 1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee45535",
   "metadata": {},
   "source": [
    "Scoring: openness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2905f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openness_score(landmarks):\n",
    "    if landmarks[15].visibility > 0.65 and landmarks[16].visibility > 0.65:\n",
    "        return abs(landmarks[15].x - landmarks[16].x)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd42349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaning_score(landmarks):\n",
    "    visible = all(landmarks[i].visibility > 0.5 for i in [11, 12, 23, 24])\n",
    "    if not visible:\n",
    "        return None\n",
    "    mid_shoulder_x = (landmarks[11].x + landmarks[12].x) / 2\n",
    "    mid_hip_x = (landmarks[23].x + landmarks[24].x) / 2\n",
    "    return mid_shoulder_x - mid_hip_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a28f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fidgeting_score(landmark_sequence):\n",
    "    total = 0\n",
    "    valid_pairs = 0\n",
    "    for i in range(1, len(landmark_sequence)):\n",
    "        for idx in [15, 16]:\n",
    "            if landmark_sequence[i][idx].visibility > 0.4 and landmark_sequence[i-1][idx].visibility > 0.4:\n",
    "                dx = landmark_sequence[i][idx].x - landmark_sequence[i-1][idx].x\n",
    "                dy = landmark_sequence[i][idx].y - landmark_sequence[i-1][idx].y\n",
    "                dist = np.sqrt(dx**2 + dy**2)\n",
    "                total += dist\n",
    "                valid_pairs += 1\n",
    "    return total if valid_pairs > 0 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbc81ed",
   "metadata": {},
   "source": [
    "Count hand gestures: need to modify a bit to fit the length of video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74dc36cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hand_gestures(landmark_sequence, threshold=0.05):\n",
    "    count = 0\n",
    "    for i in range(1, len(landmark_sequence)):\n",
    "        for idx in [15, 16]:  # Left and right wrists\n",
    "            if landmark_sequence[i][idx].visibility > 0.5 and landmark_sequence[i-1][idx].visibility > 0.5:\n",
    "                dx = landmark_sequence[i][idx].x - landmark_sequence[i-1][idx].x\n",
    "                dy = landmark_sequence[i][idx].y - landmark_sequence[i-1][idx].y\n",
    "                dist = np.sqrt(dx**2 + dy**2)\n",
    "                if dist > threshold:\n",
    "                    count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e32459ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posture_score(landmarks):\n",
    "    # Shoulders aligned and stacked over hips\n",
    "    if not all(landmarks[i].visibility > 0.5 for i in [11, 12, 23, 24]):\n",
    "        return None\n",
    "    shoulder_diff_y = abs(landmarks[11].y - landmarks[12].y)\n",
    "    torso_angle = abs(((landmarks[11].x + landmarks[12].x)/2) - ((landmarks[23].x + landmarks[24].x)/2))\n",
    "    return (1 - (shoulder_diff_y + torso_angle))  # closer to 1 is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "588147c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def space_occupation_score(landmark_sequence):\n",
    "    total_motion = 0\n",
    "    valid_frames = 0\n",
    "\n",
    "    for i in range(1, len(landmark_sequence)):\n",
    "        for idx in [15, 16]:  # Wrists\n",
    "            lm1 = landmark_sequence[i][idx]\n",
    "            lm0 = landmark_sequence[i - 1][idx]\n",
    "            if lm1.visibility > 0.5 and lm0.visibility > 0.5:\n",
    "                dx = lm1.x - lm0.x\n",
    "                dy = lm1.y - lm0.y\n",
    "                dist = np.sqrt(dx**2 + dy**2)\n",
    "                total_motion += dist\n",
    "                valid_frames += 1\n",
    "\n",
    "    return total_motion / valid_frames if valid_frames > 0 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76283f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fidgeting_score2(landmark_sequence, jitter_threshold=0.01):\n",
    "    jitter_count = 0\n",
    "    valid_frames = 0\n",
    "\n",
    "    for i in range(1, len(landmark_sequence)):\n",
    "        for idx in [15, 16]:  # wrists\n",
    "            lm1 = landmark_sequence[i][idx]\n",
    "            lm0 = landmark_sequence[i - 1][idx]\n",
    "            if lm1.visibility > 0.5 and lm0.visibility > 0.5:\n",
    "                dx = lm1.x - lm0.x\n",
    "                dy = lm1.y - lm0.y\n",
    "                dist = np.sqrt(dx**2 + dy**2)\n",
    "\n",
    "                # Only count very small movements (jitter)\n",
    "                if dist < jitter_threshold:\n",
    "                    jitter_count += 1\n",
    "\n",
    "                valid_frames += 1\n",
    "\n",
    "    # Return % of frames that had tiny twitchy motion\n",
    "    return (jitter_count / valid_frames) if valid_frames > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c82434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fidgeting_score3(landmark_sequence, jitter_threshold=0.01):\n",
    "    jitter_ratio = 0\n",
    "    movement_total = 0\n",
    "    valid = 0\n",
    "\n",
    "    for i in range(1, len(landmark_sequence)):\n",
    "        for idx in [15, 16]:\n",
    "            lm1 = landmark_sequence[i][idx]\n",
    "            lm0 = landmark_sequence[i - 1][idx]\n",
    "            if lm1.visibility > 0.5 and lm0.visibility > 0.5:\n",
    "                dx = lm1.x - lm0.x\n",
    "                dy = lm1.y - lm0.y\n",
    "                dist = np.sqrt(dx**2 + dy**2)\n",
    "                if dist < jitter_threshold:\n",
    "                    jitter_ratio += 1\n",
    "                movement_total += dist\n",
    "                valid += 1\n",
    "\n",
    "    if valid == 0:\n",
    "        return None\n",
    "\n",
    "    # Mix of average motion and jitter frequency\n",
    "    return (jitter_ratio / valid) + 0.3 * (movement_total / valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "592f19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Combine Scores with Confidence Filtering ---\n",
    "def interpret_pose(landmark_sequence):\n",
    "    if not landmark_sequence:\n",
    "        return None\n",
    "\n",
    "    current = landmark_sequence[-1]\n",
    "    scores = {}\n",
    "    valid_scores = []\n",
    "    space_score = space_occupation_score(landmark_sequence)\n",
    "    if space_score is not None:\n",
    "        space_norm = normalize(space_score, 0.001, 0.02)  # Tune thresholds\n",
    "        space_norm=10*space_norm\n",
    "        scores[\"Space Occupation\"] = space_norm\n",
    "        valid_scores.append(space_norm)\n",
    "\n",
    "\n",
    "    lean_score = leaning_score(current)\n",
    "    if lean_score is not None:\n",
    "        lean_norm = 10 - normalize(abs(lean_score), 0, 0.2)\n",
    "        lean_norm=10*lean_norm\n",
    "        scores[\"Leaning Stability\"] = lean_norm\n",
    "\n",
    "        valid_scores.append(lean_norm)\n",
    "\n",
    "    fidg_score = fidgeting_score3(landmark_sequence)\n",
    "    if fidg_score is not None:\n",
    "        fidg_norm = 10 - normalize(fidg_score, 0.005, 0.05)\n",
    "        fidg_norm=10*fidg_norm\n",
    "        scores[\"Composure Score\"] = fidg_norm\n",
    "\n",
    "        valid_scores.append(fidg_norm)\n",
    "\n",
    "    gestures = count_hand_gestures(landmark_sequence)\n",
    "    gesture_norm = normalize(gestures, 1, 20)  # higher is more expressive\n",
    "    gesture_norm=10*gesture_norm\n",
    "    scores[\"Hand Gesture Activity\"] = gesture_norm\n",
    "    if gesture_norm < 3:\n",
    "\n",
    "        valid_scores.append(gesture_norm)\n",
    "        valid_scores.append(gesture_norm)  # counts double\n",
    "    else:\n",
    "        valid_scores.append(gesture_norm)\n",
    "\n",
    "\n",
    "    posture = posture_score(current)\n",
    "    if posture is not None:\n",
    "        posture_norm = normalize(posture, 0.7, 1.0)\n",
    "        posture_norm=10*posture_norm\n",
    "        scores[\"Posture Alignment\"] = posture_norm\n",
    "\n",
    "        valid_scores.append(posture_norm)\n",
    "\n",
    "    if valid_scores:\n",
    "        low_scores = [s for s in valid_scores if s <= 5 and s!=0]\n",
    "    if len(low_scores) >= 1:\n",
    "        max_low = max(low_scores)\n",
    "        valid_scores.append(max_low)  # count max low score twice\n",
    "    if valid_scores:\n",
    "        scores[\"Overall Body Language Score\"] = round(sum(valid_scores) / len(valid_scores), 1)\n",
    "\n",
    "    return scores if scores else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2210298b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Space Occupation': 50.0,\n",
       " 'Composure Score': 0.0,\n",
       " 'Hand Gesture Activity': 100.0,\n",
       " 'Overall Body Language Score': 50.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Analyze a Specific Video File ---\n",
    "def analyze_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    landmark_sequence = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose_model.process(image_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            landmark_sequence.append(results.pose_landmarks.landmark)\n",
    "\n",
    "    cap.release()\n",
    "    return interpret_pose(landmark_sequence)\n",
    "video_path = \"/Users/theaalfon/Desktop/TestVid.MOV\"\n",
    "analyze_video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c1efb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/Users/theaalfon/Desktop/TestVid.MOV\"\n",
    "\n",
    "def analyze_score(video_path):\n",
    "    scores = analyze_video(video_path)\n",
    "    print(\"\\nPose Analysis Results:\")\n",
    "    if scores:\n",
    "        for key, value in scores.items():\n",
    "            print(f\"{key}: {value}%\")\n",
    "    else:\n",
    "        print(\"No valid pose detected in the video.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c57d924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pose Analysis Results:\n",
      "Space Occupation: 100.0%\n",
      "Leaning Stability: 71.0%\n",
      "Composure Score: 0.0%\n",
      "Hand Gesture Activity: 100.0%\n",
      "Posture Alignment: 68.0%\n",
      "Overall Body Language Score: 67.8%\n"
     ]
    }
   ],
   "source": [
    "video_path = \"/Users/theaalfon/Downloads/tedtalk.mov\"\n",
    "analyze_score(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa013bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pose Analysis Results:\n",
      "Space Occupation: 67.0%\n",
      "Composure Score: 0.0%\n",
      "Hand Gesture Activity: 100.0%\n",
      "Overall Body Language Score: 55.7%\n"
     ]
    }
   ],
   "source": [
    "video_path =\"/Users/theaalfon/Downloads/comedian.MP4\"\n",
    "analyze_score(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34d7bcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pose Analysis Results:\n",
      "Space Occupation: 100.0%\n",
      "Leaning Stability: 95.0%\n",
      "Composure Score: 0.0%\n",
      "Hand Gesture Activity: 100.0%\n",
      "Posture Alignment: 72.0%\n",
      "Overall Body Language Score: 73.4%\n"
     ]
    }
   ],
   "source": [
    "video_path =\"/Users/theaalfon/Downloads/comedian2.MP4\"\n",
    "analyze_score(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b01c48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pose Analysis Results:\n",
      "Space Occupation: 100.0%\n",
      "Leaning Stability: 99.0%\n",
      "Composure Score: 0.0%\n",
      "Hand Gesture Activity: 100.0%\n",
      "Posture Alignment: 95.0%\n",
      "Overall Body Language Score: 78.8%\n"
     ]
    }
   ],
   "source": [
    "video_path =\"/Users/theaalfon/Downloads/comedian3.MP4\"\n",
    "analyze_score(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47cda253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pose Analysis Results:\n",
      "Space Occupation: 94.0%\n",
      "Leaning Stability: 88.0%\n",
      "Composure Score: 0.0%\n",
      "Hand Gesture Activity: 16.0%\n",
      "Posture Alignment: 91.0%\n",
      "Overall Body Language Score: 57.8%\n"
     ]
    }
   ],
   "source": [
    "video_path =\"/Users/theaalfon/Downloads/anxious.MP4\"\n",
    "analyze_score(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26cf25b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pose Analysis Results:\n",
      "Space Occupation: 9.0%\n",
      "Composure Score: 0.0%\n",
      "Hand Gesture Activity: 89.0%\n",
      "Overall Body Language Score: 32.7%\n"
     ]
    }
   ],
   "source": [
    "video_path =\"/Users/theaalfon/Downloads/nervous.mp4\"\n",
    "analyze_score(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab416c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pose Analysis Results:\n",
      "Space Occupation: 31.0%\n",
      "Leaning Stability: 43.0%\n",
      "Composure Score: 0.0%\n",
      "Hand Gesture Activity: 100.0%\n",
      "Posture Alignment: 31.0%\n",
      "Overall Body Language Score: 41.0%\n"
     ]
    }
   ],
   "source": [
    "video_path =\"/Users/theaalfon/Downloads/anxious2.MP4\"\n",
    "analyze_score(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57305f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pose Analysis Results:\n",
      "No valid pose detected in the video.\n"
     ]
    }
   ],
   "source": [
    "video_path =\"/Users/theaalfon/Downloads/default.MOV\"\n",
    "analyze_score(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cbdd9e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_video(video_path, output_path=\"annotated_output.mp4\", display=False):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Output writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose_model.process(image_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                results.pose_landmarks,\n",
    "                mp_pose.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=1)\n",
    "            )\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        if display:\n",
    "            cv2.imshow('Pose Detection', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    if display:\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    print(f\"\\n✅ Annotated video saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c673b98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Annotated video saved to: annotated_output.mp4\n"
     ]
    }
   ],
   "source": [
    "# Run pose landmark overlay and export\n",
    "annotate_video(\"/Users/theaalfon/Downloads/anxious2.MP4\", display=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "non_verbal_comm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
